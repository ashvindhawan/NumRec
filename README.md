# numc

Here's what I did in project 4:

Henry's commentary on Tasks 1, 2, and 3, goes here


After finally finishing Task 3 which culminated in completing the Matrix61c_get and Matrix61c_set methods, we were ready to improve the speed of our operations in Task 4. We broke up our approach into a few phases: applying OpenMP to parallelize computation, implementing SIMD instructions, and attempting to use loop unrolling and blocking. 
We started by trying to use OpenMP to speed up operations. Given that we knew optimizing matrix multiply would be a challenge due to the method’s complexity, coupled with the fact that it was a large part of the overall project grade, we started first by applying simple OpenMP parallelization techniques like “pragma omp parallel for” to the matrix multiplication. However, as expected, this only marginally improved our method performance. We then moved on to implementing SIMD instructions to operate on multiple data points at once. This was something we struggled mightily with understanding. However, we were able to utilize the resources at our disposal to forge a path forward. We found past CS61C lecture slides, specifically Prof. Weaver’s lecture slides from Spring 2019, particularly helpful in understanding how to take our initial algorithm and iteratively make it faster. Through these resources we were able to implement SIMD functionality through the methods of _mm256_loadu_pd, _mm256_broadcast_sd, and _mm256_fmadd_pd. However, we still did not see the significant speedup that we were expecting. We realized that this was because our SIMD instructions all still depended on each other sequentially—our SIMD add could not complete until our SIMD load and broadcast were done. Thus, we realized we had to do another level of optimization: loop unrolling. We did our best to unroll our inner loops to reduce dependencies between our SIMD functions. It was at this point that we saw the most dramatic increase in our matrix operation speed. Moreover, we attempted one last measure to increase our operation speed on multiply and get over the hump: blocking. Something that we learned about by watching the aforementioned previous CS61C lectures, we tried to take advantage of cache locality to improve our performance. At first, we saw a significant improvement; after implementing blocking our speedup once again increased significantly. However, we were not able to figure out how to deal with tail cases of loop unrolling after blocking. Since we did already see a significant speedup on mul_matrix, we decided to table this issue for the time being and move on to optimizing the rest of our code. 
Since we had already implemented SIMD and OpenMP parallelization on mul_matrix, it became relatively easy to implement these on our other methods, namely add_matrix, sub_matrix, neg_matrix, and abs_matrix. This allowed us to gain more points from speeding up a wider array of commonly use methods. Additionally, we did one more thing to try to speed up operations; we decided to modify allocate_matrix to allocate contiguous memory. Our initial implmentation of allocate_matrix only allocated memory row-by-row, prohibiting us from taking advantage of the cache when doing matrix operation. We hoped that this addition would speed up the operation by reducing the amount of calls to memory.
Overall, we enjoyed the experience of physically modifying our code to better interact with hardware that we have learned about in this class such as caches, cores, and processors. The interactivity allowed us to become more comfortable in code optimization and managing parallelism. We hope to use these skills in future work so that we can add all these different ways to optimize our programs to our arsenal.
-